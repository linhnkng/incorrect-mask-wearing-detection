{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xpUjIewq3tu"
   },
   "source": [
    "# Correctly/Incorrectly Worn Face Mask Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWE8awgKC8o0"
   },
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6zicpNvAm1lk",
    "outputId": "231a64eb-4302-40ea-ac30-605be034e468"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/nguyen_l5/opt/anaconda3/envs/myenv/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nguyen_l5/opt/anaconda3/envs/myenv/bin/jupyter\r\n"
     ]
    }
   ],
   "source": [
    "!which jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "9iBBd-AIE7JA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import GlobalAveragePooling2D, Dropout, Dense, Activation, BatchNormalization, Conv2D, MaxPool2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "cWMjE50WC_v3"
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (300, 300) # resizing to reduce number of parameters\n",
    "IMG_DIR = '/Users/nguyen_l5/Downloads/data'\n",
    "BATCH_SIZE = 64 # small batch size to prevent Colab from crashing because of RAM shortage\n",
    "NUM_CLASSES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rc-3ZAFZB_tX"
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "YTLuV07F22B-",
    "outputId": "ae5e4723-8623-439b-926f-a4336a874488"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nguyen_l5/opt/anaconda3/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/Users/nguyen_l5/opt/anaconda3/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>category</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>correct/00585_Mask.jpg</td>\n",
       "      <td>correct</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>incorrect/05832_Mask_Chin.jpg</td>\n",
       "      <td>Chin</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>correct/00371_Mask.jpg</td>\n",
       "      <td>correct</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>maskoff/0_0_baojianfeng_0226.jpg</td>\n",
       "      <td>maskoff</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2719</th>\n",
       "      <td>incorrect/00223_Mask_Nose_Mouth.jpg</td>\n",
       "      <td>Nose_Mouth</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 filename    category    set\n",
       "858                correct/00585_Mask.jpg     correct  train\n",
       "2531        incorrect/05832_Mask_Chin.jpg        Chin  train\n",
       "434                correct/00371_Mask.jpg     correct  train\n",
       "1110     maskoff/0_0_baojianfeng_0226.jpg     maskoff  train\n",
       "2719  incorrect/00223_Mask_Nose_Mouth.jpg  Nose_Mouth  train"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = []\n",
    "for fold in os.listdir(IMG_DIR):\n",
    "    if fold == 'incorrect':\n",
    "        for filename in os.listdir(f'{IMG_DIR}/{fold}'):\n",
    "            dataset.append((f'{fold}/{filename}', filename[11:].split('.')[0]))\n",
    "    elif fold == 'maskoff':\n",
    "        for filename in os.listdir(f'{IMG_DIR}/{fold}'):\n",
    "            dataset.append((f'{fold}/{filename}', fold))\n",
    "    else:\n",
    "        for filename in os.listdir(f'{IMG_DIR}/{fold}'):\n",
    "            dataset.append((f'{fold}/{filename}', fold))\n",
    "\n",
    "df = pd.DataFrame(dataset, columns = ['filename', 'category'])\n",
    "# using train_test_split to split the dataset into train and test sets with a ratio of 80-20\n",
    "df_train, df_test = train_test_split(df, random_state = 123, stratify = df.category, test_size = 0.2)\n",
    "df_train['set'] = 'train'\n",
    "df_test['set'] = 'test'\n",
    "df = df_train.append(df_test)\n",
    "df.to_csv('/Users/nguyen_l5/Downloads/dataset.csv', index = False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_H38tu0J7QnM",
    "outputId": "83183cb6-c06e-4ffc-80c8-e175b0dc701f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "correct       950\n",
       "Mouth_Chin    896\n",
       "Chin          825\n",
       "maskoff       708\n",
       "Nose_Mouth    672\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distribution of data among classes\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "fjQDkpG6FL1N"
   },
   "outputs": [],
   "source": [
    "train_df = df[df['set'] == 'train'].reset_index(drop = True)\n",
    "test_df = df[df['set'] == 'test'].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1kdRwiVdoX3"
   },
   "source": [
    "## Traditional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4A-E-Yijdq6E",
    "outputId": "431d8b5e-ddcd-4d5c-95b6-8ada0ca904d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3240 validated image filenames belonging to 5 classes.\n",
      "Found 811 validated image filenames belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# creating a data generator for both the train and test set that adds noise to data\n",
    "def add_noise(img):\n",
    "    '''Add random noise to an image'''\n",
    "    VARIABILITY = 8\n",
    "    deviation = VARIABILITY*random.random()\n",
    "    noise = np.random.normal(0, deviation, img.shape)\n",
    "    img += noise\n",
    "    np.clip(img, 0., 255.)\n",
    "    return img\n",
    "\n",
    "\n",
    "training_datagen = ImageDataGenerator(\n",
    "    brightness_range=[0.2, 1.6],\n",
    "    rescale = 1./255,\n",
    "    rotation_range=5,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2, \n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    "    preprocessing_function=add_noise)\n",
    "\n",
    "train_generator = training_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    IMG_DIR,\n",
    "    x_col = 'filename',\n",
    "    y_col = 'category',\n",
    "    #color_mode = 'rgb', # traditional rgb channels\n",
    "    class_mode = 'categorical',\n",
    "    color_mode = 'grayscale',\n",
    "    batch_size = BATCH_SIZE,\n",
    "    seed = 123, # setting a seed to ensure consistency\n",
    "    target_size = IMAGE_SIZE)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "      rescale = 1./255)\n",
    "\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    IMG_DIR,\n",
    "    x_col = 'filename',\n",
    "    y_col = 'category',\n",
    "    #color_mode = 'rgb', # traditional rgb channels\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical',\n",
    "    batch_size = BATCH_SIZE,\n",
    "    target_size = IMAGE_SIZE,\n",
    "    shuffle = False,\n",
    "    seed = 123 # setting a seed to ensure consistency\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yW40Am1bmt_L",
    "outputId": "3508a972-4250-40cf-a79f-957d7adbf312"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 298, 298, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 149, 149, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 145, 145, 64)      51264     \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 72, 72, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 68, 68, 128)       204928    \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 34, 34, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 16, 16, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 32768)             0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32768)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               16777728  \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,314,437\n",
      "Trainable params: 17,314,437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# creating the network architecture\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation = 'relu', input_shape=IMAGE_SIZE+(1,)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (5,5), activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (5,5), activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax') # 5 because there are 5 classes\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "VBoKxuUyYS2Z"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "INIT_LR = 1e-4 # for the Adam optimizer\n",
    "\n",
    "# compiling the model\n",
    "model.compile(\n",
    "    optimizer = keras.optimizers.Adam(\n",
    "        learning_rate = INIT_LR, \n",
    "        decay = INIT_LR / EPOCHS),\n",
    "    loss = keras.losses.categorical_crossentropy,\n",
    "    metrics = [\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQneiUEnmzoa",
    "outputId": "652ff3ef-39c9-4334-91a0-ac60138cfded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f929a9c2b00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f929a9c2b00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.5512 - accuracy: 0.2694WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f928a074950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f928a074950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "51/51 [==============================] - 161s 3s/step - loss: 1.5512 - accuracy: 0.2694 - val_loss: 1.1993 - val_accuracy: 0.5610\n",
      "Epoch 2/40\n",
      "15/51 [=======>......................] - ETA: 1:41 - loss: 1.3321 - accuracy: 0.4295"
     ]
    }
   ],
   "source": [
    "# fitting the model\n",
    "\n",
    "CNN = model.fit(\n",
    "    train_generator, \n",
    "    epochs = EPOCHS, \n",
    "    batch_size = BATCH_SIZE,\n",
    "    validation_data = validation_generator,\n",
    "    validation_batch_size = BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZlvQtCh-JBWB",
    "outputId": "572e53d5-6bac-4a6f-e99a-4f7596e0e551"
   },
   "outputs": [],
   "source": [
    "# saving the model\n",
    "model.save('/Users/nguyen_l5/Downloads/model_weights_1113_changingDataGen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvncF893am5J"
   },
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-13 04:32:21.954828: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "new_model = tf.keras.models.load_model('/Users/nguyen_l5/Downloads/model_weights_1112')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 298, 298, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 149, 149, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 145, 145, 64)      51264     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 72, 72, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 68, 68, 128)       204928    \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 34, 34, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 16, 16, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 32768)             0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32768)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               16777728  \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,315,013\n",
      "Trainable params: 17,315,013\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_df['category'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_classes = []\n",
    "validation_images = []\n",
    "\n",
    "for i in range( -(-validation_generator.samples // validation_generator.batch_size)):\n",
    "    batch = validation_generator.next()\n",
    "    expected = np.argmax(batch[1], axis = 1) \n",
    "    validation_classes.extend(expected)\n",
    "    validation_images.extend(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classes = []\n",
    "test_images = []\n",
    "\n",
    "for i in range(len(test_df)):\n",
    "    expected = np.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "pred_dir = '/Users/nguyen_l5/Desktop/9.jpg'\n",
    "img = image.load_img(pred_dir, target_size=(300, 300))\n",
    "img_tensor = image.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "img_tensor /= 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n"
     ]
    }
   ],
   "source": [
    "print(new_model.predict(img_tensor).argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the restored model\n",
    "loss, acc = new_model.evaluate(test_df, test_labels, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100 * acc))\n",
    "\n",
    "print(new_model.predict(test_df).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "aUVoUlTNLz_W",
    "outputId": "93bb202a-ff22-4e65-9617-886a23b204b4"
   },
   "outputs": [],
   "source": [
    "def show_accuracy(history):\n",
    "  acc = history.history['accuracy']\n",
    "  val_acc = history.history['val_accuracy']\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "  epochs = range(1, len(acc)+1)\n",
    "  print(\"acc: \", acc)\n",
    "  print(\"val_acc: \", val_acc)\n",
    "  print(\"loss: \", loss)\n",
    "  print(\"val_loss: \", val_loss)\n",
    "  \n",
    "  fig, ax1 = plt.subplots()\n",
    "  fig.tight_layout()\n",
    "  ax1.axis(ymin = 0.3, ymax = 1.0)\n",
    "  ax1.set_xlabel('epochs')\n",
    "  ax1.set_ylabel('accurary')\n",
    "  ax1.plot(epochs, acc, 'r--', label='Training accuracy')\n",
    "  ax1.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "  ax2 = ax1.twinx() \n",
    "  ax2.set_ylabel('loss')\n",
    "  ax2.axis(ymin = 0.0, ymax = 2.7)\n",
    "  ax2.plot(epochs, loss, 'r--', label='Training')\n",
    "  ax2.plot(epochs, val_loss, 'b', label='Validation')\n",
    "  plt.title('Training and validation accuracy / loss')\n",
    "  plt.legend(loc=\"lower left\")#\"best\")#0\n",
    "  plt.show()\n",
    "\n",
    "show_accuracy(CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "zaHKGvC5Jhhh",
    "outputId": "db5dec5c-2a32-4e61-d5ad-369bd6f32db7"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes_map,\n",
    "                          normalize=False,\n",
    "                          title='Confusion Matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap = cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes_map))\n",
    "    inv_map = {v: k for k, v in classes_map.items()}\n",
    "    labels = inv_map.values()\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    if normalize:\n",
    "        cm = np.around(cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis], 2)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color = \"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "\n",
    "\n",
    "validation_classes = []\n",
    "validation_images = []\n",
    "\n",
    "for i in range( -(-validation_generator.samples // validation_generator.batch_size)):\n",
    "   batch = validation_generator.next()\n",
    "   expected = np.argmax(batch[1], axis = 1) \n",
    "   validation_classes.extend(expected)\n",
    "   validation_images.extend(batch[0])\n",
    "\n",
    "validation_classes = np.array(validation_classes)\n",
    "validation_images = np.array(validation_images)\n",
    "\n",
    "Y_pred = model.predict(validation_images)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "\n",
    "print(classification_report(validation_classes, y_pred,\n",
    "                            target_names = ['CHIN', 'MOUTH_CHIN', 'NOSE_MOUTH', 'CORRECT', 'MASKOFF']))\n",
    "\n",
    "cfs_mt = confusion_matrix(validation_classes, y_pred)\n",
    "classes = {'CHIN': 0, 'MOUTH_CHIN': 1, 'NOSE_MOUTH': 2, 'CORRECT': 3, 'MASKOFF': 4}\n",
    "plot_confusion_matrix(cfs_mt, classes_map = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(validation_generator.next()[1], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import silence_tensorflow.auto\n",
    "#from tensorflow.python.keras.models import load_model\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "config = ConfigProto()\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "keras_model = tf.keras.models.load_model('/Users/nguyen_l5/Downloads/model_weights_1112')\n",
    "#keras_model = load_model('model.h5')\n",
    "cam = cv2.VideoCapture(0) \n",
    "classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')\n",
    "\n",
    "label = {\n",
    "    0: {\"name\": \"CHIN\", \"color\": (51, 153, 255), \"id\": 0},\n",
    "    1: {\"name\": \"MOUTH_CHIN\", \"color\": (255, 255, 0), \"id\": 1},\n",
    "    2: {\"name\": \"NOSE_MOUTH\", \"color\": (0,0,255), \"id\": 2},\n",
    "    3: {\"name\": \"CORRECT\", \"color\": (0, 102, 51), \"id\": 3},\n",
    "    4: {\"name\": \"MASKOFF\", \"color\": (255, 153, 255), \"id\": 4},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "while True:\n",
    "    status, frame = cam.read()\n",
    "\n",
    "    if not status:\n",
    "        break\n",
    "\n",
    "    if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "        break\n",
    "    gray = Image.fromarray(frame, 'RGB')\n",
    "    #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #gray = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    faces = classifier.detectMultiScale(frame)\n",
    "\n",
    "    for x,y,w,h in faces:\n",
    "        color = (0,0,0)\n",
    "        gray_face = gray[y:y+h+50, x:x+w+50]\n",
    "\n",
    "        if gray_face.shape[0] >= 200 and gray_face.shape[1] >= 200:\n",
    "\n",
    "            gray_face = cv2.resize(gray_face, (300, 300))\n",
    "            gray_face = gray_face / 255\n",
    "            gray_face = np.expand_dims(gray_face, axis=0)\n",
    "            gray_face = gray_face.reshape((1, 300, 300, 1))\n",
    "            pred = np.argmax(keras_model.predict(gray_face))\n",
    "            classification = label[pred][\"name\"]\n",
    "            color = label[pred][\"color\"]\n",
    "\n",
    "            cv2.rectangle(frame, (x,y), (x+w, y+h), color, label[pred][\"id\"])\n",
    "\n",
    "            cv2.putText(frame, classification, (x, y + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n",
    "            cv2.putText(frame, f\"{len(faces)} detected face\",(20,20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Mostrando o frame\n",
    "    cv2.imshow(\"Cam\", frame)\n",
    "    \n",
    "    # Bam Esc de thoat\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9248b93cb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f9248b93cb0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras import models\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "model = tf.keras.models.load_model('/Users/nguyen_l5/Downloads/model_weights_1112')\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "        status, frame = video.read()\n",
    "\n",
    "        #Convert the captured frame into RGB\n",
    "        im = Image.fromarray(frame, 'RGB')\n",
    "        \n",
    "        faces = classifier.detectMultiScale(frame)\n",
    "\n",
    "        #Resizing into dimensions you used while training\n",
    "        for x,y,w,h in faces:\n",
    "            color = (0,0,0)\n",
    "            im = im.resize((300,300))\n",
    "            img_array = np.array(im)\n",
    "\n",
    "            #Expand dimensions to match the 4D Tensor shape.\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "            #Calling the predict function using keras\n",
    "            prediction = np.argmax(model.predict(img_array))#[0][0]\n",
    "            classification = label[prediction][\"name\"]\n",
    "            color = label[prediction][\"color\"]\n",
    "            \n",
    "            cv2.rectangle(frame, (x,y), (x+w, y+h), color, label[prediction][\"id\"])\n",
    "\n",
    "            cv2.putText(frame, classification, (x, y + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n",
    "            cv2.putText(frame, f\"{len(faces)} detected face\",(20,20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 2, cv2.LINE_AA)\n",
    "\n",
    "            #print(prediction)\n",
    "\n",
    "        cv2.imshow(\"Prediction\", frame)\n",
    "        key=cv2.waitKey(1) & 0xFF\n",
    "        if key == 27:\n",
    "            break\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(keras_model.predict(gray_face))\n",
    "            classification = label[pred][\"name\"]\n",
    "            color = label[pred][\"color\"]\n",
    "\n",
    "            cv2.rectangle(frame, (x,y), (x+w, y+h), color, label[pred][\"id\"])\n",
    "\n",
    "            cv2.putText(frame, classification, (x, y + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n",
    "            cv2.putText(frame, f\"{len(faces)} detected face\",(20,20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 2, cv2.LINE_AA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0Nck1YxQFw0p",
    "_2B_jAsHojLv",
    "MxfOi1h12uWp"
   ],
   "name": "correctly/incorrectly worn face mask detection (cleaned)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
