{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xpUjIewq3tu"
   },
   "source": [
    "# Correctly/Incorrectly Worn Face Mask Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWE8awgKC8o0"
   },
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/nguyen_l5/opt/anaconda3/envs/myenv/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "9iBBd-AIE7JA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import GlobalAveragePooling2D, Dropout, Dense, Activation, BatchNormalization, Conv2D, MaxPool2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import itertools\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cWMjE50WC_v3"
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (300, 300) # resizing to reduce number of parameters\n",
    "IMG_DIR = '/Users/nguyen_l5/Downloads/data' #change the directory accordingly\n",
    "BATCH_SIZE = 64\n",
    "NUM_CLASSES = 5 # 5 classes within the accumulated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rc-3ZAFZB_tX"
   },
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "YTLuV07F22B-",
    "outputId": "ae5e4723-8623-439b-926f-a4336a874488"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nguyen_l5/opt/anaconda3/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/Users/nguyen_l5/opt/anaconda3/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>category</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>correct/00585_Mask.jpg</td>\n",
       "      <td>correct</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>incorrect/05832_Mask_Chin.jpg</td>\n",
       "      <td>Chin</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>correct/00371_Mask.jpg</td>\n",
       "      <td>correct</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>maskoff/0_0_baojianfeng_0226.jpg</td>\n",
       "      <td>maskoff</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2719</th>\n",
       "      <td>incorrect/00223_Mask_Nose_Mouth.jpg</td>\n",
       "      <td>Nose_Mouth</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 filename    category    set\n",
       "858                correct/00585_Mask.jpg     correct  train\n",
       "2531        incorrect/05832_Mask_Chin.jpg        Chin  train\n",
       "434                correct/00371_Mask.jpg     correct  train\n",
       "1110     maskoff/0_0_baojianfeng_0226.jpg     maskoff  train\n",
       "2719  incorrect/00223_Mask_Nose_Mouth.jpg  Nose_Mouth  train"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = []\n",
    "for fold in os.listdir(IMG_DIR):\n",
    "    if fold == 'incorrect':\n",
    "        for filename in os.listdir(f'{IMG_DIR}/{fold}'):\n",
    "            dataset.append((f'{fold}/{filename}', filename[11:].split('.')[0]))\n",
    "    elif fold == 'maskoff':\n",
    "        for filename in os.listdir(f'{IMG_DIR}/{fold}'):\n",
    "            dataset.append((f'{fold}/{filename}', fold))\n",
    "    else:\n",
    "        for filename in os.listdir(f'{IMG_DIR}/{fold}'):\n",
    "            dataset.append((f'{fold}/{filename}', fold))\n",
    "\n",
    "df = pd.DataFrame(dataset, columns = ['filename', 'category'])\n",
    "# using train_test_split to split the dataset into train and test sets with a ratio of 80-20\n",
    "df_train, df_test = train_test_split(df, random_state = 123, stratify = df.category, test_size = 0.2)\n",
    "df_train['set'] = 'train'\n",
    "df_test['set'] = 'test'\n",
    "df = df_train.append(df_test)\n",
    "df.to_csv('/Users/nguyen_l5/Downloads/dataset.csv', index = False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_H38tu0J7QnM",
    "outputId": "83183cb6-c06e-4ffc-80c8-e175b0dc701f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "correct       950\n",
       "Mouth_Chin    896\n",
       "Chin          825\n",
       "maskoff       708\n",
       "Nose_Mouth    672\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distribution of data among classes\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fjQDkpG6FL1N"
   },
   "outputs": [],
   "source": [
    "train_df = df[df['set'] == 'train'].reset_index(drop = True)\n",
    "test_df = df[df['set'] == 'test'].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1kdRwiVdoX3"
   },
   "source": [
    "## Traditional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4A-E-Yijdq6E",
    "outputId": "431d8b5e-ddcd-4d5c-95b6-8ada0ca904d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3240 validated image filenames belonging to 5 classes.\n",
      "Found 811 validated image filenames belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# creating a data generator for both the train and test set that adds noise to data\n",
    "def add_noise(img):\n",
    "    '''Add random noise to an image'''\n",
    "    VARIABILITY = 8\n",
    "    deviation = VARIABILITY*random.random()\n",
    "    noise = np.random.normal(0, deviation, img.shape)\n",
    "    img += noise\n",
    "    np.clip(img, 0., 255.)\n",
    "    return img\n",
    "\n",
    "\n",
    "training_datagen = ImageDataGenerator(\n",
    "    brightness_range=[0.2, 1.6],  # adding brightness range to images\n",
    "    rescale = 1./255,            # rescaling\n",
    "    rotation_range=5,           # rotating by 5 degrees\n",
    "    width_shift_range=0.1,     # shifting width\n",
    "    height_shift_range=0.1,   # shifting height\n",
    "    shear_range=0.2,         # adding image distortion along an axis\n",
    "    zoom_range=0.2,         # adding zoom range\n",
    "    horizontal_flip=True,  # flipping the image\n",
    "    fill_mode=\"nearest\",\n",
    "    preprocessing_function=add_noise)\n",
    "\n",
    "train_generator = training_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    IMG_DIR,\n",
    "    x_col = 'filename',\n",
    "    y_col = 'category',\n",
    "    class_mode = 'categorical',\n",
    "    color_mode = 'grayscale', #changing to greyscale channel\n",
    "    batch_size = BATCH_SIZE,\n",
    "    seed = 123, # setting a seed to ensure consistency\n",
    "    target_size = IMAGE_SIZE)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "      rescale = 1./255)\n",
    "\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    IMG_DIR,\n",
    "    x_col = 'filename',\n",
    "    y_col = 'category',\n",
    "    color_mode = 'grayscale', #changing to greyscale channel\n",
    "    class_mode = 'categorical',\n",
    "    batch_size = BATCH_SIZE,\n",
    "    target_size = IMAGE_SIZE,\n",
    "    shuffle = False,\n",
    "    seed = 123 # setting a seed to ensure consistency\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yW40Am1bmt_L",
    "outputId": "3508a972-4250-40cf-a79f-957d7adbf312"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 298, 298, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 149, 149, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 145, 145, 64)      51264     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 72, 72, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 68, 68, 128)       204928    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 34, 34, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 16, 16, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 32768)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32768)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               16777728  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,314,437\n",
      "Trainable params: 17,314,437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-15 06:16:13.167532: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# creating the network architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation = 'relu', input_shape=IMAGE_SIZE+(1,)), # 1 for greyscale, 3 for RGB\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (5,5), activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (5,5), activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation = 'relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),                  # adding a drop-out rate of 50%\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax') # 5 because there are 5 classes\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VBoKxuUyYS2Z"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "INIT_LR = 1e-4 # for the Adam optimizer\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3) # stopping if there is no significant difference\n",
    "                                                                        # in loss after 3 epochs\n",
    "\n",
    "# compiling the model\n",
    "model.compile(\n",
    "    optimizer = keras.optimizers.Adam(\n",
    "        learning_rate = INIT_LR, \n",
    "        decay = INIT_LR / EPOCHS),\n",
    "    loss = keras.losses.categorical_crossentropy,\n",
    "    metrics = [\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oQneiUEnmzoa",
    "outputId": "652ff3ef-39c9-4334-91a0-ac60138cfded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fc02907b7a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fc02907b7a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "51/51 [==============================] - ETA: 0s - loss: 1.3603 - accuracy: 0.4086WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fbdb0271dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fbdb0271dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "51/51 [==============================] - 161s 3s/step - loss: 1.3603 - accuracy: 0.4086 - val_loss: 0.9591 - val_accuracy: 0.6215\n",
      "Epoch 2/40\n",
      "51/51 [==============================] - 161s 3s/step - loss: 1.1430 - accuracy: 0.5207 - val_loss: 0.6194 - val_accuracy: 0.7534\n",
      "Epoch 3/40\n",
      "51/51 [==============================] - 159s 3s/step - loss: 1.0277 - accuracy: 0.5818 - val_loss: 0.4822 - val_accuracy: 0.8126\n",
      "Epoch 4/40\n",
      "51/51 [==============================] - 160s 3s/step - loss: 0.9491 - accuracy: 0.6204 - val_loss: 0.4291 - val_accuracy: 0.8496\n",
      "Epoch 5/40\n",
      "51/51 [==============================] - 160s 3s/step - loss: 0.8525 - accuracy: 0.6605 - val_loss: 0.4978 - val_accuracy: 0.7978\n",
      "Epoch 6/40\n",
      "51/51 [==============================] - 161s 3s/step - loss: 0.7838 - accuracy: 0.7043 - val_loss: 0.2985 - val_accuracy: 0.9088\n",
      "Epoch 7/40\n",
      "51/51 [==============================] - 161s 3s/step - loss: 0.7015 - accuracy: 0.7543 - val_loss: 0.3015 - val_accuracy: 0.8927\n",
      "Epoch 8/40\n",
      "51/51 [==============================] - 160s 3s/step - loss: 0.6635 - accuracy: 0.7556 - val_loss: 0.2512 - val_accuracy: 0.9162\n",
      "Epoch 9/40\n",
      "51/51 [==============================] - 161s 3s/step - loss: 0.6123 - accuracy: 0.7926 - val_loss: 0.3031 - val_accuracy: 0.8668\n",
      "Epoch 10/40\n",
      "51/51 [==============================] - 161s 3s/step - loss: 0.5648 - accuracy: 0.7997 - val_loss: 0.1851 - val_accuracy: 0.9309\n",
      "Epoch 11/40\n",
      "51/51 [==============================] - 160s 3s/step - loss: 0.5128 - accuracy: 0.8275 - val_loss: 0.1871 - val_accuracy: 0.9322\n",
      "Epoch 12/40\n",
      "51/51 [==============================] - 159s 3s/step - loss: 0.4765 - accuracy: 0.8444 - val_loss: 0.1744 - val_accuracy: 0.9273\n",
      "Epoch 13/40\n",
      "51/51 [==============================] - 160s 3s/step - loss: 0.4283 - accuracy: 0.8525 - val_loss: 0.1773 - val_accuracy: 0.9309\n",
      "Epoch 14/40\n",
      "51/51 [==============================] - 159s 3s/step - loss: 0.4168 - accuracy: 0.8525 - val_loss: 0.1879 - val_accuracy: 0.9383\n",
      "Epoch 15/40\n",
      "51/51 [==============================] - 158s 3s/step - loss: 0.3573 - accuracy: 0.8830 - val_loss: 0.1735 - val_accuracy: 0.9309\n",
      "Epoch 16/40\n",
      "51/51 [==============================] - 160s 3s/step - loss: 0.3408 - accuracy: 0.8873 - val_loss: 0.1823 - val_accuracy: 0.9223\n",
      "Epoch 17/40\n",
      "51/51 [==============================] - 160s 3s/step - loss: 0.3332 - accuracy: 0.8855 - val_loss: 0.1425 - val_accuracy: 0.9408\n",
      "Epoch 18/40\n",
      "51/51 [==============================] - 158s 3s/step - loss: 0.3092 - accuracy: 0.8975 - val_loss: 0.1138 - val_accuracy: 0.9618\n",
      "Epoch 19/40\n",
      "51/51 [==============================] - 156s 3s/step - loss: 0.3147 - accuracy: 0.8883 - val_loss: 0.1243 - val_accuracy: 0.9507\n",
      "Epoch 20/40\n",
      "51/51 [==============================] - 157s 3s/step - loss: 0.2994 - accuracy: 0.9056 - val_loss: 0.1348 - val_accuracy: 0.9581\n",
      "Epoch 21/40\n",
      "51/51 [==============================] - 160s 3s/step - loss: 0.2807 - accuracy: 0.9034 - val_loss: 0.0991 - val_accuracy: 0.9679\n",
      "Epoch 22/40\n",
      "51/51 [==============================] - 158s 3s/step - loss: 0.2572 - accuracy: 0.9120 - val_loss: 0.0881 - val_accuracy: 0.9716\n",
      "Epoch 23/40\n",
      "51/51 [==============================] - 158s 3s/step - loss: 0.2621 - accuracy: 0.9086 - val_loss: 0.0750 - val_accuracy: 0.9741\n",
      "Epoch 24/40\n",
      "51/51 [==============================] - 159s 3s/step - loss: 0.2463 - accuracy: 0.9194 - val_loss: 0.1276 - val_accuracy: 0.9593\n",
      "Epoch 25/40\n",
      "51/51 [==============================] - 159s 3s/step - loss: 0.2557 - accuracy: 0.9179 - val_loss: 0.0732 - val_accuracy: 0.9716\n",
      "Epoch 26/40\n",
      "51/51 [==============================] - 159s 3s/step - loss: 0.2087 - accuracy: 0.9330 - val_loss: 0.0822 - val_accuracy: 0.9766\n",
      "Epoch 27/40\n",
      "51/51 [==============================] - 157s 3s/step - loss: 0.1994 - accuracy: 0.9315 - val_loss: 0.0646 - val_accuracy: 0.9778\n",
      "Epoch 28/40\n",
      "51/51 [==============================] - 158s 3s/step - loss: 0.2097 - accuracy: 0.9352 - val_loss: 0.0646 - val_accuracy: 0.9815\n",
      "Epoch 29/40\n",
      "51/51 [==============================] - 159s 3s/step - loss: 0.1894 - accuracy: 0.9361 - val_loss: 0.0800 - val_accuracy: 0.9803\n",
      "Epoch 30/40\n",
      "51/51 [==============================] - 156s 3s/step - loss: 0.1791 - accuracy: 0.9386 - val_loss: 0.0517 - val_accuracy: 0.9815\n",
      "Epoch 31/40\n",
      "51/51 [==============================] - 156s 3s/step - loss: 0.1677 - accuracy: 0.9509 - val_loss: 0.0442 - val_accuracy: 0.9864\n",
      "Epoch 32/40\n",
      "51/51 [==============================] - 159s 3s/step - loss: 0.1646 - accuracy: 0.9451 - val_loss: 0.0456 - val_accuracy: 0.9840\n",
      "Epoch 33/40\n",
      "51/51 [==============================] - 160s 3s/step - loss: 0.1673 - accuracy: 0.9435 - val_loss: 0.0473 - val_accuracy: 0.9852\n",
      "Epoch 34/40\n",
      "51/51 [==============================] - 157s 3s/step - loss: 0.1418 - accuracy: 0.9562 - val_loss: 0.0522 - val_accuracy: 0.9815\n",
      "Epoch 35/40\n",
      "51/51 [==============================] - 158s 3s/step - loss: 0.1417 - accuracy: 0.9543 - val_loss: 0.0483 - val_accuracy: 0.9840\n",
      "Epoch 36/40\n",
      "51/51 [==============================] - 159s 3s/step - loss: 0.1438 - accuracy: 0.9556 - val_loss: 0.0491 - val_accuracy: 0.9864\n",
      "Epoch 37/40\n",
      "51/51 [==============================] - 158s 3s/step - loss: 0.1381 - accuracy: 0.9556 - val_loss: 0.0782 - val_accuracy: 0.9778\n",
      "Epoch 38/40\n",
      "51/51 [==============================] - 159s 3s/step - loss: 0.1248 - accuracy: 0.9605 - val_loss: 0.0458 - val_accuracy: 0.9852\n",
      "Epoch 39/40\n",
      "51/51 [==============================] - 159s 3s/step - loss: 0.1221 - accuracy: 0.9602 - val_loss: 0.0483 - val_accuracy: 0.9840\n",
      "Epoch 40/40\n",
      "51/51 [==============================] - 159s 3s/step - loss: 0.1173 - accuracy: 0.9630 - val_loss: 0.0417 - val_accuracy: 0.9827\n"
     ]
    }
   ],
   "source": [
    "# fitting the model\n",
    "\n",
    "CNN = model.fit(\n",
    "    train_generator, \n",
    "    epochs = EPOCHS, \n",
    "    batch_size = BATCH_SIZE,\n",
    "    validation_data = validation_generator,\n",
    "    validation_batch_size = BATCH_SIZE,\n",
    "    callbacks = [callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZlvQtCh-JBWB",
    "outputId": "572e53d5-6bac-4a6f-e99a-4f7596e0e551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7fc029095320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7fc029095320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fc039dfa560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7fc039dfa560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-13 07:24:35.218990: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/nguyen_l5/Downloads/model_weights_1113_changingDataGen/assets\n"
     ]
    }
   ],
   "source": [
    "# saving the model\n",
    "model.save('/Users/nguyen_l5/Downloads/model_weights_changingDataGen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvncF893am5J"
   },
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('/Users/nguyen_l5/Downloads/model_weights_changingDataGen') # loading model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 298, 298, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 149, 149, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 145, 145, 64)      51264     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 72, 72, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 68, 68, 128)       204928    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 34, 34, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 16, 16, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 32768)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32768)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               16777728  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,314,437\n",
      "Trainable params: 17,314,437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "aUVoUlTNLz_W",
    "outputId": "93bb202a-ff22-4e65-9617-886a23b204b4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rw/pxymp48w83d24hrnsbxpzdjw000gvn/T/ipykernel_8769/436561716.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mshow_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCNN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# can only be used with model trained within the opening time of the notebook as this graph calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                    \u001b[0;31m# calls for the loss and accuracy during training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CNN' is not defined"
     ]
    }
   ],
   "source": [
    "def show_accuracy(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc)+1)\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.tight_layout()\n",
    "    ax1.axis(ymin = 0.3, ymax = 1.0)\n",
    "    ax1.set_xlabel('epochs')\n",
    "    ax1.set_ylabel('accurary')\n",
    "    ax1.plot(epochs, acc, 'r--', label = 'Training accuracy')\n",
    "    ax1.plot(epochs, val_acc, 'b', label = 'Validation accuracy')\n",
    "    ax2 = ax1.twinx() \n",
    "    ax2.set_ylabel('loss')\n",
    "    ax2.axis(ymin = 0.0, ymax = 2.7)\n",
    "    ax2.plot(epochs, loss, 'r--', label = 'Training')\n",
    "    ax2.plot(epochs, val_loss, 'b', label = 'Validation')\n",
    "    \n",
    "    plt.title('Training and validation accuracy / loss')\n",
    "    plt.legend(loc = \"lower left\")\n",
    "    plt.show()\n",
    "\n",
    "show_accuracy(CNN) # can only be used with model trained within the opening time of the notebook as this graph calls\n",
    "                   # calls for the loss and accuracy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "zaHKGvC5Jhhh",
    "outputId": "db5dec5c-2a32-4e61-d5ad-369bd6f32db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f8591222dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f8591222dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CHIN       0.60      0.02      0.04       165\n",
      "  MOUTH_CHIN       0.00      0.00      0.00       179\n",
      "  NOSE_MOUTH       0.17      0.99      0.28       135\n",
      "     CORRECT       0.00      0.00      0.00       190\n",
      "     MASKOFF       0.00      0.00      0.00       142\n",
      "\n",
      "    accuracy                           0.17       811\n",
      "   macro avg       0.15      0.20      0.06       811\n",
      "weighted avg       0.15      0.17      0.05       811\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nguyen_l5/opt/anaconda3/envs/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/nguyen_l5/opt/anaconda3/envs/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/nguyen_l5/opt/anaconda3/envs/myenv/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAEmCAYAAAAjsVjMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABF0UlEQVR4nO2dd5hUVdKH3x8gIKKCAkqUpCCgsgLqKiBmUMx5DRhWMa15zd/K6rrmxYhpzQEVE4IomFhXXSWJCVFAUBlMmFEQGOr745wemqGnp5mZTlAvz33oe+6551TfmamurlOnSmaG4ziOkz1q5VsAx3Gc1R1XtI7jOFnGFa3jOE6WcUXrOI6TZVzROo7jZBlXtI7jOFnGFa1TUEhaW9IoST9JGlGNcY6QNK4mZcsHkp6XNCjfcjjVwxWtUyUk/UnSJEkLJH0ZFULvGhj6IGAjYEMzO7iqg5jZw2a2ew3IswKS+kkySU+Va98qto/PcJwhkh6qrJ+ZDTCz+6sorlMguKJ1VhlJZwM3AP8kKMU2wDBg3xoYfhPgEzNbWgNjZYtvge0lbZjUNgj4pKYmUMD/PlcXzMwPPzI+gPWBBcDBafrUIyjiefG4AagXr/UD5gLnAN8AXwLHxmt/BxYDS+IcxwNDgIeSxm4LGFAnnh8DfAr8AswGjkhqfz3pvu2BicBP8f/tk66NBy4H3ojjjAOaVPDeEvLfDpwa22rHtr8B45P63gh8AfwMTAb6xPb+5d7nu0lyXBHlWAh0jG1/jtdvA55IGv9q4GVA+f698CP94Z+YzqryR6A+8HSaPhcD2wHdga2AbYBLkq5vTFDYLQnK9FZJjc3sUoKV/JiZNTSzu9MJImkd4CZggJmtS1CmU1P02wB4LvbdEPgX8Fw5i/RPwLFAM6AucG66uYEHgKPj6z2ADwkfKslMJDyDDYBHgBGS6pvZC+Xe51ZJ9xwFnAisC3xWbrxzgC0lHSOpD+HZDbKodZ3CxRWts6psCMy39F/tjwAuM7NvzOxbgqV6VNL1JfH6EjMbQ7DqOlVRnmVAN0lrm9mXZvZhij57ATPM7EEzW2pmw4HpwN5Jfe41s0/MbCHwOEFBVoiZvQlsIKkTQeE+kKLPQ2b2XZzzeoKlX9n7vM/MPoz3LCk33m/AkYQPioeAv5jZ3ErGcwoAV7TOqvId0ERSnTR9WrCiNfZZbCsbo5yi/g1ouKqCmNmvwKHAScCXkp6T1DkDeRIytUw6/6oK8jwInAbsRAoLX9I5kj6KERQ/Eqz4JpWM+UW6i2Y2geAqEeEDwSkCXNE6q8r/gEXAfmn6zCMsaiVow8pfqzPlV6BB0vnGyRfNbKyZ7QY0J1ipd2UgT0KmkirKlOBB4BRgTLQ2y4hf7c8HDgEam1kjgn9YCdErGDOtG0DSqQTLeB5wXpUld3KKK1pnlTCznwiLPrdK2k9SA0lrSRog6ZrYbThwiaSmkprE/pWGMlXAVKCvpDaS1gcuTFyQtJGkfaKv9neCC6I0xRhjgM1iSFodSYcCXYDRVZQJADObDexI8EmXZ11gKSFCoY6kvwHrJV3/Gmi7KpEFkjYD/kFwHxwFnCepe9Wkd3KJK1pnlTGzfwFnExa4viV83T0NeCZ2+QcwCXgPeB+YEtuqMteLwGNxrMmsqBxrERaI5gHfE5TeKSnG+A4YGPt+R7AEB5rZ/KrIVG7s180slbU+FnieEPL1GeFbQLJbILEZ4ztJUyqbJ7pqHgKuNrN3zWwGcBHwoKR61XkPTvaRL1g6juNkF7doHcdxsowrWsdxnCzjitZxHCfLuKJ1HMfJMumCzp0c0qRJE2uzSdt8i7ECqrxLXpg6+7t8i7AS3dttWHknh88+m8P8+fNr7Fer9nqbmC1dmLaPLfx2rJn1r6k5q4Ir2gKhzSZtee3NCfkWYwXq1C7MLzxNjyi8rIFvPOwpYzNhh2171uh4tnQR9ToflrbPondurmw3XtZxRes4TvEiQIX63Ws5rmgdxyluatXOtwSV4orWcZwiRlAE+dFd0TqOU7wIt2gdx3Gyi9xH6ziOk3XcdeA4jpNN5K4Dx3GcrOLhXY7jODnAXQeO4zjZRFC78F0Hhf9R4GTEokWL6Nd7O/7Y6w/0+sMWXHHZkHyLBMC4sS+wZddOdO3ckWuvuSqncw87aXs+vfMQ3r5unxXaB/fvzJSh+zHhun25/IgeAOy0RXNeu3Igb127D69dOZC+XTdONWRWyeezqohClGkFRLBo0x2VDSHdI+kbSR8ktT0maWo85kiaGtvbSlqYdO32TMR0i3Y1oV69eox+4SUaNmzIkiVL2H3nvuy2R3+22Xa7vMlUWlrKmaefynPPv0jLVq3ovV0vBg7ch827dMnJ/A//ZxZ3jJ3Onaf2Lmvr03Vj9urZmu3++iyLly6jyXr1Afjul9855JqX+eqHhWzeuhHPXLQbnU4eUdHQNU6+n1WxyLQyNbIYdh9wC0kl483s0LIZpOsJhTUTzDKz7qsygVu0qwmSaNgwVMhesmQJS5YsQXleJJg4YQIdOnSkXfv21K1bl4MPPYzRo0bmbP43PvqaHxb8vkLbn3frxL9GfsDipcsAmP/zIgDem/M9X/0QskB99MWP1F+rFnXr5O7PI9/PqlhkSomU/qgEM3uNUHMuxdASoZLx8OqI6Ip2NaK0tJTtt9ma9q03ZqdddqXXNtvmVZ5580po1ap12XnLlq0oKaluhe/q0bH5emzfuRmv/GNPnr90D7busHJ6w3233YR353xfpoxzQSE+q0KUKSWVuw6aSJqUdJy4CqP3Ab6OxTATtJP0jqT/xLLyleKKthySNpb0qKRZkqZJGiNps2T/Tew3RNK58fV9kg6Kr8dLmpTUr6ek8bmQvXbt2rw5YQrTZ33O5IkTmfbhB5XflEVSFf7Mt5Vdp7ZotE49dr5kDJc8NJn7z9xxheudWzXisj/14Iy73sqpXIX4rApRppVQdB2kO2C+mfVMOu5chRkOZ0Vr9kugjZn9gVAJ+hFJ66W8MwlXtEnErwlPA+PNrIOZdSGUdN5oFYdqJmlAjQuYIY0aNaJP3x15cdzYfIkABAto7tzlFbZLSubSokWLPEoEJd/9xrMTPgNg8qz5LFsGTdYN1bpbbNCA4ef0Y/Cw/zL7619yKlchPqtClCkl1XQdVDys6gAHEMrdA2Bmv8fy9ZjZZGAWsFllY7miXZGdgCVmVraSaGZTgS8qvCM11wKX1KBclfLtt9/y448/ArBw4UJefeVlNuvUKZcirETPXr2YOXMGc2bPZvHixYx47FH2GrhP5TdmkdETP2fHrs2B4EaoW6cW83/5nfUbrMUTF+zCpcOn8NbH3+ZcrkJ8VoUo08qo2lEHadgVmG5mc8tmk5pKqh1ftwc2BT6tbCCPOliRbsDkCq51SIR4RDYGrqug7/+A/SXtBFRoGkVf0YkArVu3WWVhk/n6qy8Z/OdjKS0tZdmyZRxw4MEM2HNgtcasLnXq1GHojbew9157UFpayqBjjqNL1645m/+e0/vSp8tGbLhufaYPO4h/jpjKg6/OZNjJ2/P2dfuweOkyBg97HYAT+29O+43W5fwDt+L8A7cCYN8rXixbLMs2+X5WxSLTStRA9i5Jw4F+BF/uXOBSM7sbOIyVF8H6ApdJWgqUAieZWcqFtBXmSOWHWVORdDrQzszOKtfeFhhtZt2S2oYAC8zsOkn3xetPRH/sucB6wMXA+cB1ZtYv3dxb9+hpXsomMwqxlM23XsomI3bYtieTJ0+qMUdvrUabWL0+56fts2j0qZPNrGZr6KwihfmXlD8+BHrUxEBm9gpQH8hfIKvjrAlUvhiWd1zRrsgrQD1JJyQaJPUCNqnieFcA59WEYI7jVECWFsNqEle0SVjwo+wP7BbDuz4EhgDzqjjeGCD3KyuOs6agrC6G1Ri+GFYOM5tH2AlSnm7l+g1Jen1M0ut+5frViCvCcZzUqFZhKNN0uKJ1HKdoCeloC8M9kA5XtI7jFC+KR4HjitZxnCJG1HLXgeM4TnZx14HjOE42EaiWK1rHcZysIeQWreM4TrZxRes4jpNlfDHMcRwnm3h4l+M4TvZx14HjOE4WUZHE0Ra+hI7jOOlQJUdlt0v3SPomuS5grAlYImlqPPZMunahpJmSPpa0RyYiukVbIIjCTbRdaCz9ak6+RXAKBdXIYth9wC3AA+Xah5rZClVUJHUhVF7oCrQAXpK0mZmVppvA/7IdxylqJKU9KsPMXgMqLUcT2Rd4NBZpnA3MBLap7CZXtI7jFC2JDQvVUbRpOE3Se9G10Di2tWTFYq1zY1taXNE6jlO8xC246Q5C0cVJSceJGYx8G9AB6A58CVy/fMaVqLTwovtoHccpajKwWuevanFGM/s6afy7gNHxdC7QOqlrKzKowOIWreM4RU02XAeSmied7g8kIhKeBQ6TVE9SO2BToNLy1W7ROo5T1FQ3e5ek4UA/gothLnAp0E9Sd4JbYA4wGMDMPpT0ODANWAqcWlnEAbiidRyniKmBBS/M7PAUzXen6X8FocJ1xriidRynqCmGnWGuaB3HKW4KP9WBK1rHcYobTyrjOI6TRSSoVQSlbArfueFkzLixL7Bl10507dyRa6+5Kt/iAPmV6fbz9+GzZ85l0r0nl7U9eOmBvPXvwbz178FMf/QM3vr3YADWqlOLOy7Yh4n3nsTbdw+mT/dNcior+M+vamR1Z1iN4RbtakJpaSlnnn4qzz3/Ii1btaL3dr0YOHAfNu/SZY2V6cHnp3L7UxP490X7l7Ud9fcny15fdcru/PTrIgCOG9gDgF7H3k7TRg145poj6D34LqzSPT81Q76fVbHIlIoC0aVpcYt2NWHihAl06NCRdu3bU7duXQ4+9DBGjxq5Rsv0xnuf8/0vCyu8fuBOXXj8pRCH3rltU16dPBuAb3/8jZ8WLKJHpxY5kRPy/6yKRaaViK6DdEch4Ip2NWHevBJatVq+M7Bly1aUlJTkUaLClCnBDlu24evvf2VWSUja9P6sr9i7dydq1xabbNyIP2zWglbN1s+ZPIX4rApRpvKI4lC07jpYTbAU33Hz7Z8qRJkSHLLrFox4uSzPM/ePeYfObZryxh0n8vnXP/LWh1+wtHRZzuQpxGdViDKlolCUaTpyYtFKMkkPJp3XkfStpNFJbfvFlGTTJb0vab+ka+Ml9Uw6byvpA0l7JGVAXxAznk+V9ICkfsnjx/vuk3RQGjnXknSVpBlx/AmSBsRrcyQ1SepbNr6kYyTdEl8PkfSbpGZJfRdU8dFlTMuWrZg7d3n2tpKSubRokbuvvqkoRJkAatcW+/bpzBOvLle0paXGebeOZbs/38EhFz9Go4b1mTn3u5zJVIjPqhBlWgkFH226oxDIlevgV6CbpLXj+W5A2XcQSVsB1wH7mllnYB/gOklbphvUzMaaWXcz6w5MAo6I50dXUc7LgeZANzPrBuwNrFuFceYD51RRhirRs1cvZs6cwZzZs1m8eDEjHnuUvQbuk0sRikImgJ17tOeTz+dT8u0vZW1r16tDg/prhes927O0dBnTP5ufM5kK8VkVokzlEdlJKlPT5NJ18DywF/AEcDgwHOgTr50L/DNmLMfMZku6EvgrcFQuhJPUADgBaGdmv0c5vgYer8Jw9wDHSLrazCrM3B7zYp4I0LpNmypMs5w6deow9MZb2HuvPSgtLWXQMcfRpWvXao1ZXfIt0/1/O4A+3dvSZP0GzBxxFpffO577x7zDwTt34/EktwFA08brMOraI1lmxrxvf+H4K57OmZyQ/2dVLDKtTOH4YdORS0X7KPC3+HV7S4IySijargSLNplJwKnVnLOPpKlJ521YnleyPB2Bz83s5zTjvSopkamnITC9gn4LCO/vDEImoJSY2Z3AnQA9evSsdiBR/wF70n/AnpV3zCH5lGnQZU+lbD/xqpVXzj//6ie2OurWbIuUFv/5VY1CsVrTkTNFa2bvSWpLsGbHlLssVs5SntyWSgllopj+a2YDywaU7stI2IrZyczmx7H6ESzxirgJmCrp+jR9HMepDgXkh01HrsO7niVYrsPLtX8IlM+AvjUh5yPAd0DjpGsbEPygNclMoI2kqvhkV8LMfgQeAU6pifEcx1mZYgnvyrWivQe4zMzeL9d+HXBhtHiJ/1/E8jo944Ejtfw7wiDg1ZoUzMx+I+SgvElS3ShHc0lHVmPYfxESBnsYneNkiWJYDMupojWzuWZ2Y4r2qcD5wChJ04FRwHmxHYIf8xfgXUnvEvyj5X26NcElwLfANEkfAM/E8yoR3QxPA/VqRDrHcVakBnaGxSq338S/+UTbtTHU9D1JT0tqFNvbSlqYFFZ6e0ZipgpKdnJPjx497Y23J+VbjKKg8S5/z7cIK/HDyxWueTpJ7LBtTyZPnlRjZmbDVp2s22l3pu3z9oX9JqcrziipL2EB+4EY1omk3YFXzGyppKsBzOz8+G17dKJfpvgWXMdxipjqZ+8ys9eA78u1jTOzpfH0LUK12yqzRvoOJT0NtCvXfL6Zjc2HPI7jVJ0M3ANNJCV/XbwzhlZmynHAY0nn7SS9A/wMXGJm/61sgDVS0ZrZ/pX3chyn4MksvGt+OtdB2uGliwnVbh+OTV8CbczsO0k9gGckda0k/n7NVLSO46weJLbgZmVsaRAwENjF4mJW3DWa2Dk6WdIsYDPCBqsKcUXrOE5Rk41YWUn9CZFQO8bQz0R7U+B7MyuV1B7YFPi0svFc0TqOU9RU16KVNBzoR/DlziVsm7+QEJb5Yhz/LTM7CegLXCZpKVAKnJQun0kCV7SO4xQtUvV3f5nZ4Sma766g75PAk6mupcMVreM4RU2BbP5KS6VxtJIOTuz/l3SJpKckbZ190RzHcSqnlpT2KAQy2bDwf2b2i6TewB7A/cBt2RXLcRyncrQaFWdM5F/dC7jNzEYCdbMnkuM4TubUUvqjEMhE0ZZIugM4BBgjqV6G9zmO42SdYsjelcli2CFAf+A6M/tRUnNCiRmnBjFSVx3NJ4XyS1qejn13yLcIToEgKBg/bDoyUbTNgefM7PdYVWBL4IFsCuU4jpMpheIeSEcmLoAngVJJHQmxZe0IlQMcx3Hyi9IvhBXKYlgmFu2ymJPxAOAGM7s5Zq5xHMfJK6uT62CJpMOBo4G9Y9ta2RPJcRwnc4pAz2bkOjgW+CNwhZnNltQOeCi7YjmO41ROscTRVmrRmtk04PSk89nAVdkUynEcJ1NWC9eBpE2BK4EuQP1Eu5m1z6JcjuM4GVH4ajYz18G9hC23S4GdCKFdD2ZTKMdxnEwQULuW0h6FQCaKdm0ze5lQMfczMxsC7JxdsRzHcTKgkl1hhbLpJhNFu0hSLWCGpNMk7Q80y7JcjuM4GVHdxTBJ90j6RtIHSW0bSHpR0oz4f+OkaxdKminpY0l7ZCRjBn3OBBoQFsR6AEcBgzIZ3Mktg084jk1abkTP7lvkW5Qyxo19gS27dqJr545ce01u11Av268L48/vy1OnbVfWdtou7Xny1G0Zccq23DHoDzRdd8X8SBuvX4+3L+nHoB3a5FRWyO+zqohClCmZEEdb7aQy9xHSDCRzAfCymW0KvBzPkdQFOAzoGu8ZJql2ZRNUqmjNbKKZLTCzuWZ2rJkdYGZvZSS+k1OOOvoYnhn9fL7FKKO0tJQzTz+VkaOe5533pjHi0eF8NG1azuYf+c48Tn5gxb01977+GQfe+jYHD3ub/3w8n5P6rbime96ATrw+47ucyZgg38+qWGRKRXVdB2b2GlC+HM2+hJSwxP/3S2p/1Mx+jxFYM4FtKpujwqgDSaMIuU4qEm6fygZ3ckvvPn35bM6cfItRxsQJE+jQoSPt2gdldvChhzF61Eg279IlJ/NP/uxHWjSqv0Lbr7+Xlr1eu27tFX7Bd968KXN/+I2Fi5flRL5k8v2sikWm8khQOzt+2I3M7EsAM/tSUsJd2hJINjTnxra0pAvvuq7KIjoOMG9eCa1atS47b9myFRMmvJ1HiQJ/2bUD+3Rvzi+LlnL8PZMBWHutWhzXexNOuP8djtlhk5zLVIjPqhBlSkUGeraJpORy4Hea2Z1VnS5FW6Vp9ypUtGb2HwBJ6wALzWxZPK9NqA7pOGlJlfaxEFaBb35pFje/NIvj+7bl8O1aM+yVTzll5w48+L/PWbi4tPIBskAhPqtClCkVGcg038x6ruKwX0tqHq3Z5sA3sX0u0DqpXytgXmWDZbIY9jJhMSzB2sBLld0kySRdn3R+rqQhSecnSpoejwmxVE7i2kBJ70h6V9I0SYNj+xBJJZKmJh2NKpi/X5Th+KS2P8S2c+O5Yh20GZI+kfSqpK5J/ReUG/MYSbdIujhp/tKk16dHGc8td98cSU0qe2arGy1btmLu3C/KzktK5tKiRYs8SrQiY979il27hG+EW7Raj7N235QXzt6BI//YmhP6tuPwbVvlTJZCfFaFKFN5RPoY2mrE0T7L8kX/QcDIpPbDJNWL6Qg2BSZUNlgmSWXqm1mZwjGzBZIapLsh8jtwgKQrzWx+8gVJA4HBQG8zmx+LPT4jaRvgO+BOYBszmxsrOrRNun2omWXq1ngfOJTlpYMPA95Nun4qsD2wlZn9Jml34FlJXc1sUUWDmtkVwBXxvSwws+5J721IhrKt9vTs1YuZM2cwZ/ZsWrRsyYjHHuW+B/ObYbPNBmvz+fcLAdipc1Nmz/8VgGPunlzW5+Sd2vPb4qUMf3tuzuQqxGdViDKthKqfVEbScKAfwcUwF7iUkGbg8WiofQ4cDGBmH0p6HJhG2MR1qplV+jUoE0X7q6StzWxKFKoHsDCD+5YSFOZZwMXlrp0P/DWhgM1siqT7CYpvaJTru3jtd+DjDOZLxefAepI2Ipj+/YEx5eToZ2a/xbnGSXoTOIIK6rrXJJJOBE4EaN2m+uFEg478E6+9Np7v5s+nY7vWXPK3IRxz7PGV35gl6tSpw9Abb2HvvfagtLSUQcccR5euXSu/sYa4+uBu9GrXmEYN1uKlc3tz6yuf0mezJrRt0gAzY96Pi7j82ek5kycd+X5WxSJTKqq7GGZmh1dwaZcK+pcZWpmSiaI9ExghKeGHaE6wEjPhVuA9SdeUa+8KTC7XNgkYZGbfS3oW+EzSy8BoYHjCRwycJenI+PoHM9upEhmeIHwavQNMIVjaSFoPWMfMZqWQo7q/TckyAqT8vhUd8ncCbN2jZ7Xr2Nz/UIFZG0D/AXvSf8CeeZn7/BEfrNT29JRK3Wnc9uqn2RCnUvL5rCqiEGVKRhSm37g8mWTvmiipM9CJ8L6mm9mSTAY3s58lPUDY7FCZFSzi6p2Z/VnSFsCuwLnAbsAxsd+quA4AHgceAzoDwwmugozkqIBMFOIKMkqak8E9juNUgQJJZ5CWjKrZmtkSM/vAzN7PVMkmcQNwPLBOUts0wi6zZLaO7Yk53zezoQQle+AqzlmGmX0FLInjvJzU/jPBLVI+C1myHAslJW8d2gCYj+M4BYG0+iSVqRZm9j3Bqkx2Fl4DXC1pQwBJ3QkW6zBJDRWKQCboDnxWTTH+Bpyfwml9LXCTpLWjHLsCvVleE+0/wJHx2tqEisCvVlMWx3FqkBrYgpt1MvHR1gTXA6clTszsWUktgTclGfALcGSMWVsXOE/SHQR3w68sdxvAyv7P/cxsTrrJzezNCi7dDDQG3pdUCnwF7GtmCTfHGcAdkk4nuBQeiNv1HMcpEIrARZtR4m8RVuHbm9llktoAG5tZ2tgxM2uY9PprVozFxcxuI+S5LX/fL0BK73tM0TikMplj3/HA+ArGSLw24O/xSDVGCTCwknkaljtfST4za1upwI7jrDIC6hSBps3EdTCMUDMsEQLxCyGawHEcJ+9I6Y9CIBPXwbZmtrViiXEz+6HcAlFeUcgHeXW55tlmtn8+5HEcJ3dIhbPglY5My43XJoY1SWoK5D69UQWY2VhgbL7lcBwnPxSBns1I0d4EPA00k3QFcBBwSValchzHyYCQ+LvwNW0mGxYeljSZsB1NhFX+j7IumeM4TmUIamc9SLX6ZBJ10Ab4DRiV3GZmn2dTMMdxnExQERQcz8R18BzBPyugPtCOkOSl8LJLOI6zRpGoGVboZOI6WKHSX0xpODhrEjmO46wCq0vUwQrElIa9siGM4zjOqrDaWLSSzk46rUVIuvJt1iRyHMfJFK0+Fu26Sa+XEny2T2ZHHMdxnMypCYtWUidCKtUE7QmJqBoBJ7DcsLzIzMZQBdIq2rhRoaGZ/bUqgzuO42Sb6obRmtnHhCyBCZ1XQtg7cCyrnv86JRUqWkl1zGxpXPxyskyxZIovBGaOfibfIqzMpbvmW4I1EqFql7Ipxy7ALDP7rCb/HtNZtBMI/tipsbTMCELKQgDM7Kkak8JxHKcqZJZztomkSUnnd8YyUqk4jFCJJcFpko4mlLg6x8x+qIqYmfhoNyAUStyZ5fG0BriidRwn72SwBXe+mfWsrFNMlrUPcGFsug24nKDvLifk1T6uKjKmU7TNYsTBByxXsAmqXUjQcRynuogajToYAEyJ+bMTebTDPNJdhEKxVSKdoq0NNISU+9tc0TqOUxDUoCv1cJLcBpKam9mX8XR/gtFZJdIp2i/N7LKqDuw4jpNtJGpkMUxSA0IB1+Rdr9fEeoYGzKEaO2LTKVpfAnccp+CpCUVlZr8BG5ZrO6oGhgbSK9pdamoSx3GcbFD0+WhjmXDHcZyCpgh24Oas3LjjOE4WUFFs9HFF6zhO0SIyK+Wdb1zROo5T1BSDj7YYPgycDBk39gW27NqJrp07cu01V+VbHCC/Mt1+6RF89vKVTBpxUVnbFpu1ZPz95zDx8Yt44obBrLtO/bJr5x63Ox+MvJR3n/4/dv3j5jmVFfznVyUUcoSkOwoBV7SrCaWlpZx5+qmMHPU877w3jRGPDuejadPWaJkeHPUW+5566wptt/3tT1xy00h6HfJPnn31Xc4aFIJrOrffmIP32JqtD7qCfU4dxo0XHkKtHK6y5PtZFYtM5REhjjbdUQi4ol1NmDhhAh06dKRd+/bUrVuXgw89jNGjRq7RMr0xZRbf//TbCm2bbtKM1yfPBOCVt6az3y7dARjYb0tGjJ3C4iVL+Wzed8z6Yj69urXNmaz5flbFIlMqVMlRCLiiXU2YN6+EVq1al523bNmKkpKSPEpUmDJNm/UlA/uFMngH7LY1rTZqDEDLpusz96vliZlKvvmBFs3Wz5lchfisClGmVEjpj0KgKBWtpI0lPSpplqRpksZI2kxSV0mvSPpE0gxJ/6fopJF0jKRvJU2VNF3SWUnjDZFUEq9Nk3R40rX7JM2O16ZKejPp2gBJkyR9FMe8TtLFSX1Lk16fns1nYrZy+ol8+6cKUabBQx5m8CF9eePh82jYoB6Ll5QmBFupbwrxs0YhPqtClKk8xeI6KLqog6g4nwbuN7PDYlt3YCPgPuBkMxsX9y4/CZwCJBx1j5nZaZI2BD6W9ISZfRGvDTWz6yRtCkyO15bEa381syfKydENuAXYy8ymS6oDnGhmw4ArYp8FZtY9G8+hPC1btmLu3C/KzktK5tKiRYtcTF0hhSjTJ3O+Zu9Twq9DxzbNGNCnKwAl3/xIq40bl/Vr2awxX377U87kKsRnVYgyrYxQwTgIKqYYLdqdgCVmdnuiwcymApsBb5jZuNj2G3AacEH5AczsO2Am0DzFtRnAb0Dj8tfKcR5whZlNj/ctjUo2L/Ts1YuZM2cwZ/ZsFi9ezIjHHmWvgfvkS5yClalp44ZAsMwuOGEP7nridQCeG/8eB++xNXXXqsMmLTakY5umTPxgTs7kKsRnVYgypaIYXAdFZ9EC3YDJKdq7lm83s1mSGkpaL7ldUhugPvBe+UFi6Z4ZZvZNUvO1ki6Jrz80syOiHNdX/W2ApBOBEwFat2lTnaGoU6cOQ2+8hb332oPS0lIGHXMcXbp2rdaY1SXfMt1/5TH06bEpTRo1ZOYLl3P57WNouHY9Bh/aF4CRr0zlgZFvAfDRp1/x5Lh3eOfJi1lauowzr3qcZcty5zvI97MqFpnKU1PZu7KNUvlhCpno62xnZmeVax8KzDazm8q1/wC0AQ4ErgW+AToBJ5jZvbHPEEK1ywWECpj9zezleO0+YHQK18EU4FgzezeNrAvMrGEm76tHj572xtuTKu/o0LjXafkWYSV+mHhLvkUoCnbYtieTJ0+qMc24WbfudsuIF9P22aNLs8mZVFjIJsXoOvgQ6FFB+woPU1J7YIGZ/RKbHjOzrkAf4HpJGyd1H2pmnYBDgQck1Sc9FcnhOE6OKJbFsGJUtK8A9SSdkGiQ1AuYAfSWtGtsWxu4Cbim/ABm9j/gQeCMFNeeIhRiG1SJHNcCF0naLM5XK5b+cRwnh6iSfxmNIc2R9H6MEJoU2zaQ9GKMYHpRUmXrNhVSdIrWgq9jf2C3GN71ITAEmAfsC1wi6WPgfWAiITIgFVcDx0paN8W1y4CzJSWez7VJYVpTJdU1s/eAM4Hhkj4ilLlYaXHNcZzsUoOLYTuZWfckN8MFwMtmtinwMikW1jOlGBfDMLN5wCEVXO5XwT33EcK/ksdIuA6GlOs7meDHBTgmjRyjSVOwLVP/rOM4VSPhOsgS+7Jcn9wPjAfOr8pARWfROo7jLKcyx0HGStiAcZImx2gggI0SxRnj/82qKmVRWrSO4zhAzN5Vaa8mCb9r5E4zu7Ncnx3MbJ6kZsCLkqbXpJiuaB3HKVoydB3Mryy8K7oSMbNvJD0NbAN8nSg5Lqk5ITS0SrjrwHGcoqa62bskrZNYFJe0DrA7YXH7WZZHHw0Cqpy6zC1ax3GKmhpIdLMR8HQcpw7wiJm9IGki8Lik44HPgYOrOoErWsdxiprq6lkz+xTYKkX7d8Au1Rs94IrWcZyipjD2fqXHFa3jOEWLKLwcualwRes4TvFSQKkQ0+GK1nGcoqYI9KwrWsdxipnCKSmeDle0juMUNUWgZ13ROo5TvITFsHxLUTmuaAsEI3XV0XxSqF/JWu6yV75FcAqIYijO6IrWcZyipkDtgRVwRes4TvHi4V2O4zjZx10HjuM4WcQXwxzHcXKAK1rHcZws464Dx3GcLFOr8PWsK1rHcYqcIlC0XsrGcZyiJZSrqV4VXEmtJb0q6SNJH0o6I7YPkVQiaWo89qyqnG7ROo5TvKhGXAdLgXPMbEqsHTZZ0ovx2lAzu666E7hFuxox+ITj2KTlRvTsvkW+RSlj3NgX2LJrJ7p27si111yV07mvPGQL3h6yM2PO7b3SteN3bMfM6wbQuMFaADRqsBYPnbQN716xG5fu3yWncibI57OqiEKUaSWqWZ3RzL40synx9S/AR0DLmhTRFe1qxFFHH8Mzo5/PtxhllJaWcubppzJy1PO88940Rjw6nI+mTcvZ/E9Nmstxd01aqb35+vXpvdmGlPywsKzt96XLGPrCDK4aPT1n8iWT72dVLDKtTGWOAwE0kTQp6TixwtGktsAfgLdj02mS3pN0j6TGVZXSFe1qRO8+fdmg8Qb5FqOMiRMm0KFDR9q1b0/dunU5+NDDGD2qyhWbV33+T3/gx9+WrNR+8b6bc/Xoj1dI4rNwcSmT5/zA70uW5Uy+ZPL9rIpFpvKI4DpIdwDzzaxn0nFnyrGkhsCTwJlm9jNwG9AB6A58CVxfVTld0TpZY968Elq1al123rJlK0pKSvIoEezSpRlf/bSI6V/+klc5ylOIz6oQZUpJNV0HAJLWIijZh83sKQAz+9rMSs1sGXAXsE1VRSxKRSvJJD2YdF5H0reSRpfrN1LS/8q1dZI0Pq4ifiTpztjeL/l+Sf+QNFZSPUm9JU2QND0eJyb1K78yeVVsHy/p46T2g7L1PAqVVGkf85l6sf5atTh51w7cMHZG3mSoiEJ7VlCYMqWilpT2qAyFN3U38JGZ/SupvXlSt/2BD6oqY7FGHfwKdJO0tpktBHYDVvioldQI2BpYIKmdmc2Ol24irCSOjP1WWjmSdDGwA7An0Bh4BNgvrko2AcZKKjGz5+ItFa1MHmFmKzsJ1xBatmzF3LlflJ2XlMylRYsWeZOnzYYNaL3B2ow+ewcANl6/PiPP2oEDbnqT+b8szptcUHjPCgpTplTUgOrfATgKeF/S1Nh2EXC4pO6EdNFzgMFVnaBYFS3A88BewBPA4cBwoE/S9QOBUcDXwGHAlbG9OTA30cnM3k8eVNI5BAW7h5ktlHQRcF/SquR8SecBQ4DncCqkZ69ezJw5gzmzZ9OiZUtGPPYo9z34SN7k+eSrBWw75JWy8/EX7cj+N7zJDyn8uLmm0J5Vocq0EjWQJtHMXie1vh5TvZGXU5Sug8ijwGGS6gNbsnyVMEFC+Q6PrxMMBV6R9Lyks6Llm2AH4CRggJktiG1dgcnlxp4U2xOcleQi2COp/eGk9g3LvwFJJyZWQufP/zajN52OQUf+iX59t+eTTz6mY7vW3Hfv3dUeszrUqVOHoTfewt577UH3LTbnwIMPoUvXrpXfWEMMPWIrRvxlO9o1XYfXL9mJg7dplbb/+It25KJ9OnNAz5a8fslOdNyoYY4kzf+zKhaZyhOydyntUQio0MqnZIKkBWbWUNIk4FZgU2AccK6ZDZS0EUHxtjMzkzQFONrMPoj3twD6A/sCnYCtgD8C1xJcBReY2ROx79PAvWb2bNL86wOfmtmGkoYAC8q7DiSNj/Jk5DrYukdPe+OtiVV8ItmhUH5Jy9PtgsIJYUvwwVUD8i1CUbDDtj2ZPHlSjf1ibfWHHvb8q/9L26dl43qTzaxnTc1ZFYrZogV4FriOYLUmcyhBYc6WNAdoS3AfAGBm88zsHjPbl7ArpFu89DXBbTBU0k6x7UOg/A+pB1BoAYWOs0YipT8KgWJXtPcAl5X3sxJcBf3NrK2ZtSUoxsMAJPWPoRxI2hjYkKSFNDP7BDgAeCg6wm8FjomviS6Aq4Frsve2HMfJlGJwHRTzYhhmNhe4Mbkt7uxoA7yV1G+2pJ8lbQvsDtwoaVG8/Fcz+0pS56T+EyUdS7CYdwKOBO6K+6AF3GBmo7L41hzHyZDCUKXpKUpFa2YrrVKY2XhgfDxdaZ+ymW0dX74NnF3J/ZjZOILCBpgF9KpAliEVtPdL1e44Ts0hkVGsbL4pSkXrOI5TRuHrWVe0juMUN0WgZ13ROo5TzGS2zTbfuKJ1HKdoKZZy48Ue3uU4jlPwuEXrOE5R464Dx3GcbFJAu7/S4YrWcZyipVh8tK5oHccpajIpKZ5vfDHMcZyipiaSysQcKB9LminpgpqW0RWt4zhFTXUVraTahORRA4AuhMoKNVpz3hWt4zhFTQblxitjG2CmmX1qZosJRQX2rUkZXdE6jlO0JBbDquk6aAl8kXQ+lxSJqaqDL4YVCO9MmTy/Qd1an9XQcE2A+TU0Vk2xWsu09vU1MUoZq/Oz2qQGxihjypTJY9deS00q6VY/VmNJcKeZ3Zl0nkod12jpGVe0BYKZNa2psSRNynfpjvK4TJlTiHIVokwAZta/BoaZC7ROOm8FzKuBcctw14HjOGs6E4FNJbWTVJdQjeXZSu5ZJdyidRxnjcbMlko6DRgL1AbuMbMPa3IOV7SrJ3dW3iXnuEyZU4hyFaJMNYaZjQHGZGv8oiw37jiOU0y4j9ZxHCfLuKJ1nNUAFUpdbSclrmjXECStdj9rVy4gqbWkulZAPkBJnSXVaLxssbPa/fE5KyJpa0ktzWzZ6qBsJXWQdC6AmdmarGwl7QncAGyYZ1HKiDI9iC+0r0DR/+E5FSNpLeAA4EFJLVYTZbsUuFLSxVA4ylZSS0n3xWeei/l2By4H/mlmX+ZizsqQtAfwD+A8M5slae18y1QoFPsfnZMGM1sC3Ai8AdyxOli2ZvYZ0Bn4i6RLY1vela2ZlQDtgYezrWwl9QeeABqa2eTYlhMFn0ampsDNwMNm9qqk1sDzktrkU65CoWj/4JyKkbS9pH0k1TOzbwlWxhTgTkmtik3ZSuoo6WlJ+0rqamazgJ7AcflWtslzmllfYG3gkWwpPknbA9cBOwPTJL0a514iKS9f1yV1JuRBuBnYXtLBwHBgpJl9ng+ZCo2i+WNzMiNaFvcDzxCs2AeAToQthS8AwyQ1KzJl2xfoD1wCPCvpEmBHYFfgLEknQVC2uRRKUhPgSEmNkpqXRtkerWllK2lTYDvgEDObZGb7A0uSlO3SmFs1Z0Tr+k5gEzO7GXgVGAJMMrOhsU+x/J5ljTX+AaxOSGoSLdhzgUnAf4AfgOOAu4ANgHbAPbHvsrwJmwGSNpbU18zuAf4KjCR8iEwAjgWOJiQEGSbpvBzL1oSwJ74PsLuk9SQ9BUwzs2aEv62HakrZStobuB1Y18ymSaoHYGa7A4uTlG1prpRtXPi6ErjIzOZIqmNmtxAW6DpK6iNprUL/PcsFrmhXEyS1Am6QtI+ZjSR8jTsIGAGcD5wG/Ap8T/Bx1suXrKvAAOB0STsRPigWAesBv5vZrsAdwD8J1vq0XAklaXOCFfcm4cOsd3w9z8wSi3T7E6IBRtTAfHsCVwEXA9fG8X9PKFQz2wP4TdKUeF5a3TkzkKk18DfgeTN7XVJzgmuquZndBbxI+L3r6xath2CsFkRXwFxJbwP7S1piZg/GP8S/A9ea2QsEZXBd7P9NXoVOg6SWQDNCmFBd4Kh46SbCB8bBkhqY2fMEf+jTZrZQkrLtPpC0WZTrKjObIukdgrugBfB6tOqWApjZrpKqlVow/gz3B841s7cSflhJtaP1WsfMlprZXpKelLRJXDDMNj8QFuTWk3Qy4Wf0aCICwsxujFEHJxN+7xbmQKaCxRVtkRMtiwskvWpmN0saTKh5hJndJ2kZcHb8A33JzBYVuJKtRfC9ngCcDfybkJj56Pj/zcApwH6S6pvZ0wRLN+s+2rjo8yzQwcyeSJrzsahUtgPWkfSYmS2I1ydVOGBm1CG4e9aP4yWUeMJqbQD8HNsOrOZclSKpMVDLzL6TdCNwPHAMMN3Mbop96prZYjO7SlJjM1ujlSy4ol0dmA98CmwXLdk74kp4Qtk+EP15fyYsVBQ0cZFuLMFKvBS4ALgnXj6KkPl+GHA68HG8J+uLYNE18zDB//0HSXOAbc3s6yjDfZKOIrgRakm6uzq+SUk7A9+a2fuSXiBYjioXXdEYOE/SVWb2Y9XfXcYy7QOcQahYMAP4yMyulrQU2FzSAYRIg8VJlvYP2ZarKDAzP1aDg7AiPxYYGM8HA/cC+8bz9fMtYyXyrxv/rx3/bwpMJkRKbEvIE3oiIWxolxzL1oywGLdDUtu/gDlA06Q2ERbpulRzvt0JHyK94/m+hAXAXRPPJ7YfArwMbJiDZ9Af+Aj4IyGKZVvgPeDmeP0UYCjwp3z/LhXi4RZtESKpE3A9sL+F+MmmwN5ACbBLtCbuiF9n+0t6ycx+yqfM6Yhy/kfSrWZ2d3Qf3ElQLm8SFl2GECzbOsC3OZStLnAw0I0Q9QCAmZ0d3TITJfUys28taJx7qzlff8LC13Fm9oakZoSFpcbAZcDjkn4hPIdTgSPM7LvqzJkhuwD/Z2b/S7Ks+xB+bmcAtwHnAN0krWtmv+RApqLBFW2RIekPwAJgCfDvGFP6ACEr/B2STgF2in6yGyQ1MrNf8ylzOiQ1NLMFCvkLbpe0kLBteJaZnStpHYIf8hrgAjMblkv5LHwNfh1YFzhT0l1m9k68dq6kUmC6pM2tmr5vSe2Bq4H7o5JtDjwNXGPBNTGfUNywP6Gm1Z/M7IPqzJmBTN3iHJsAP0LZ5pA6ZvaTpLMJcb2LJd0K1HEluzJrfNhFMSFpL+AhoDthoQjC19enzOyOeH4/8DnQMyqxH3MsZsbEMKlH46r1q4SIgn8A65hZInHMr4TV7ceBrIctJcm2TuK1mb1LWASbC5wgaauka+cTnnnXGph2HsEVsJ6kg4CnCEr3qTjXaDO7lRC/e2YOlGwrwqaXtQkum3VjRAgWF+UIawTtoxX7s5l9n02ZipZ8+y78yOwg7DaaSViASbQ1JIQaPVKub32gcb5lruT9dAHeJUQX1Etq70NY3DuwXP86OZRtHcIH2EEpZL6QEKTfhFihJOm6qjjf+kRfb5z7SsKGkxvL9Tsc6JnD59AKeDvKtA3wJMH33yqpz5+A54AG+f6dKuTDLdrioQdh4eFtxd1GFkKITiJsw3w8sRptIYSrYFd7YxTERcAtZnaXmf0e29cys/8SYi+vlHR04h5bbkFlHQtW9AUEa27fpPZphAXHzQnKZoX8ChY1z6ogaSDB9/uKpNPi3JcBzwO/S9oh9juI4KfOuhtI0lbRcv0K+B1YaGYTCBtEdgfOlfRPSWcSPnguNLPfsi1XMeOKtsBJ+kNuR1iJhxD6BJQphX8SAvsfya10VSMq1oXADIBEEL6FbGOY2ViCpXtF9FPmBEmNJK0fvwY/SviK/qCk/eL1WmY2haCANo2yVjm0LC58XUHYQXU2cL6k/S3EnV5JCGXbW9JVBMW/n5l9VPV3mJFMawFHALcQLPgvCd+QMLNxwP8BowiLcw2Bw8zsvWzKtDrgxRmLhBhXeRFwvplNjivzWIg7PYVQwXOhxbjOQiV+cNQl+Jpftbi4Fd+PET78B5vZMEkbWm5W1BP+4nsJLoMt4+v7CNbraMLq/ihJWxMWHwdZTFFYxfk2iuNMN7MzYttBhA/Tx8zs+2j5X0rI1HW81XAJ7BQytbWQs6AVYSPC9gRlezNhU8QnLF9An2oFvPGl0HCLtnh4G3gdOFRSDzNbFpXsoQTrb2mhK1kIFmC0aG8hrOLvEy8lts/2BfaVtFEOlWw7gpX2bzM7DDiPkKfgn4R41n0JroxhhLCzC6qjZAHiz+pRQkKY42PzAYQFwamSLiMo94uAATlQsnsCD0jawczmEnz/7xH8xzsTPnAGEZ7NRYQoDCdD3KItIqLf7HhCTONEwtbTgwiLNlldga5JkuIwjyR8Jb6e8EHShBCPeYGZjcqhPMcD3czsrKS2zYEjgUVmdrlCisJFhEWfjxPvoQpztSREVXwSz48k+N+7EjY8/IlgUfciLID+2UJS8ayhUBnhKuAMM3stqX0jwiaRLsDJZvajQu6FZdVxmayJuKItMmKoTQ/CLqEvCV+/P8mvVBWjmPwkzfVdCQsq3xB8fv82s5FVVWSrKFtC4R8O7GRmJyrkT1gUr+9MSPnXz2ogbCmG511GWMV/gfCBskjSYcTVe1sepkeOnkFtgpvkBTN7RFJDgjW/HWHhrxHhw70HwX1REGVzig1XtE7WkLQxsBvwspnNS3E9oegaAr8B60WrKRcKpiPBahxJCF0aBmxnZj8rVKZIREI8DvzVqpkRKy58DQX2IywE3g88a8uTYx9NiI/+ArgrRpRknegzvxV4BXiHkMuhBbAVMItg0f5GyJVxT7at69UV99E62aQbYftq/6h0VyChTM1sgYUELDnZJhytuHsILotDzWw8wXp7I0YcJJRsH8K+/mqVyJG0LsH/OgMosVDe5WyglcJ2aszsAYI7qAU53LEZfwavEHI5vEBYqLzLzNoSFgbPiFbsla5kq45btE5WiXGgJxMKRD5hoQJE+T6J3Kq1LEfZ+CUNIqRefI2wI+tBQkjVnoRdaIkY5TPM7NkamK8nwZqtT9j9diVhK20DQsB/InZX2bZmFeqOdbCQs7hWXFRtC6xlZjOSfh4nE7beXug+2erhuQ6cGiUu9nQxsxcBLOzZv41Q1WGppFFm9lVS/8QfdSPgHEnXWJb2ysdwqVILmx/eIqT8a0ZQfkea2VkKJWHaEPzFx1mo6FrVha/NCKv2PxGKY35PiBB5GfjJzNop5DfoTFh4a2Zms6v9RtPLtD5h4at3DONaJOk2M5uT6BN/HkdGWY9yJVt9XNE6NUZUlt2BCyX9amZvShpJyBMwluDnWybpeTObl6Rk1ycUkxySRSXbkRAX+4qkYTFy4EKCL/I7oJdCNq7Hy1uUVVSy+xDCwz4jxAdvRLCW/01YbPpN0vpm9ilhy/GYKr+5VcBCIphhhEWuhcBmwBhJ1xJiemdLOh84EDg622FlawruOnBqhOhrvIqgXDYHDiXslR9pZn+LffYiFIocR0iWsigq5yeBS83s9SzKtyOh7lhdQp7bEUAHQp7bawmxovsRFoTuIli+VXJjRHfJ3YQ42MmxbRghj0M/gpV7HEH5/j3GrWYVhUoHPyReE+Jhx5jZfyU9SIhfLiF8GL0LfJFqAdOpGr4Y5lSbqGQfAZ40s4kEC/ZJglIbG/vUMrPnCItQb0UlWy/eNySbSjbyJiEhyvOExa35QE9C/oDjomzjgfFmtqSavuKNCWFqk7W8Wu0phA0nI4HZhN1mnxHSXWYVSbsDL8b/iQq3DqEKR1dC1MVphIiDg4CZrmRrFrdonWoRleyLwCQzOyCpfT1CUcGDgGEWCimmur+1mX2RI1kbEOJDBwMvEbbAHkooyTKxBsZPFGLcD+hloTptIlnOEoU8Ak8TthiXJMfsZhNJfyFY7a8Bd5jZkwpbnicRvn0cbmbPxL51zWxxtmVa03AfrVNlJHUgKI57gK0Vsjndb2Y/xHjUkcAyQl0rkpVtYrU7V0oWwMx+k/QmwaI9neAeSNQjq9YGgegWuZKQJGY0sLFC5q9RUcnWif/XIvhHS3KhZCPDgfaEPMVHRGU6XNIdwBZm9oxCYp9ScmBhr4m4onWqRIyL7QhcFP9QuxPiUpF0j4Uk0D9KGk2K8jPZDONSmlIq0WXxOmGB6q9xQWpovFZVJbsjcCPBJ/u2pPqEBa6dCO65p81sqaSDCXGyWS/FI2lLAAuZtb4HFhO2+d4OnCbpJ4J75/IYCTI22zKtybjrwFllovXzCCFb2KCk9u7AdQSL7h4z+znR33KUT1YhR8HDhI0Sn1akPKPvtA/wvYXUh9WZ82yCdXxj4qu3pCaEBa8uhBSXbwIDCco4q2kFJW1IUOZzCRsjPiMs8t1I8J83JqRCvInwITjLCngb9+qAK1qnSkS3wTDgAzM7J6m9OyH64BWCPzBnRSEldSZEDDxiZrdl0L8sD4MqyclQwf2JLcQ3E+JiL4lbWhU3AdQnhI/1Bt4nKLRZq/q+qoJCnoaXCJsjlhB8sSXAu2b2kKRjgT0ISWtyst13TcajDpyMiVtXAYgK4ySgu6Trk9qnEtLo7UmwnHIlW2uCJf24md0maS1Jjylk3UrVPxHDm0hqvcr1yJKs5aeB7RTSV1ocv070wfYi5HoYlyslG2V7hZBnYhDhA/E/hOiCAQqVfZ8ATnAlmxtc0ToZoVDp4Mq4VROAuIvpeGALhfypifYpwN7Ju41yQDfgQ2BGjM19FPjazGaU71huN9qoaJ1Xh1S5gpcq5AoeRPCR5hwze5mw/Xk88LCZ7QhcYmaLzeyXbG0OcVbGXQdORkTLbwTB33d1crRA9IteFo8P4tfprGfginO3AjY2s0lx2+g2hHy9r5rZaUn9Nga+BmrZ8t1oTxFieP9bA3IUbK5ghaTe1wM7WEz3mKufjxNwi9ZJi6S6cRV/EWFbZj3g/+JX9QQzCEX8aiX+eHOkZDsTNiBsIamBmT1ESF4zHXgz4RaQ1JewG22TZEuWsBut2koWwEJmq2uBiwkJab4A9sm3kgUwszGEhDUvSarlSjb3uEXrVIikLoSFrbUJyusdM7tH0u2xy1ALOQPaE76qH5crxSJpE4JP9gYzuztZeUg6kLCldjwhocsQ4CozezbGsZ4JTDaz/+RC1kJBUkP3yeYHV7ROSqK1OIKQrHoyyzNMfWxm58YFsGaE4P8uhD37I3Mo365A/yhLHcLK/pZAfTO7Jn5dPpqQW+B4C1tsE/c2MC+P7eQQV7ROSiRdDPxqZjcktbUlFFUcF///A7ABMN/M3snlV1JJ/Qh5XPclpPOrRUhtWJ/gwthRoarsXDN7KznsKhfyOU4yvjPMqYjmhGTUZQsnFkpR3wLsHxXWCpVgs61kFbJO1SV8AIyXdCKhHHcJwcUxzcwWSnpG0gZm9kQ52dyqcPKCK1qnDEltgDYWMmmNA7ZP8TV7JtBD0oaWo3LgUbbEjq85QDtJLwIPmFmfcv12AloTKhfkJazKccrjUQcOCqxHiAe9O34tf5uQo/SYuF01QTPCAlNOttRG+doSto7eYiFD2FmENIdXxogCJDWVNJBQtfZvloMcr46TKe6jdcqQdCUhR2spYevmfEIqwTGESqjTCLkMLrCYVi9Hch0E9DazM5PaNiaU6G4HnE/YYnoNIQphVK5kc5xMcIt2DUchR2qC8QTXwG2EJNAbEnLKzgF6EEKmzrWQratalWEzlC0xhxHcBWsl5LVQd+y/wNaESIPJhIq2o3Ihm+OsCq5o12BiCFfCVQDBL7s+sC1wPyH4fgszu8/MDiUo2dG5iC6Q1BQ4QVJD4GNgHWB9i7ldASwk6/6aUBIGM5sf//evaU5B4YthazbNCLGxXeMmhGXAhcDhhCqx9YDz486wEYnQqBwpsh0I22lrmdntkqYRCiv2S9pG2hvoRNiV5jgFiyvaNRgzey0uJo0F5gHbE3Z4tSSUxB5B2JCQ1RLYySSlKxxFKJy4i6QTzOz0GFr2tKQZwCzgROB0C5VkHadg8cUwB0l7EFbrtyRsQhgAvGFmLym3Sbs7EUqSjwNeM7PfJQ2I8nxoZndI6kOwYusB78cPC9+77xQ0rmgdoKzm1XXAdmb2k2JBwRzLsCPwKiFJzeOEOlfXEvKq1iX4Yx80M3cVOEWFK1qnDEn9CYtgnS2UpM6HDL0JyWK2JWQLa0yIfJhLULyXA3cTXMX+y+sUBa5onRWIlu2vZjY+jzIMIMTEbm9mv0RLdwuCT/YsCwmtHadocEXrpCTffs+YfetmoFf5ZNX5ls1xVhWPOnBSkm9FZmZjJC0DpkvqZGY/5DKpuOPUJG7ROgVNIbgyHKe6uKJ1igJ3FzjFjCtax3GcLOO5DhzHcbKMK1rHcZws44rWcRwny7iidXKGpFJJUyV9IGmEpAbVGOu+mBAcSf+OpdEr6ttP0vZVmGOOpCYZ9j0mJr1xnJVwRevkkoVm1t3MugGLgZOSL0qqXZVBzezPZjYtTZd+hMxkjpMXXNE6+eK/QMdobb4q6RHgfUm1JV0raaKk9yQNhrK6ZrdImibpOUIuXeK18ZJ6xtf9JU2R9K6kl2O9sZOAs6I13SfWF3syzjFR0g7x3g0ljZP0jqQ7CCkiV6L8HCmu7y3p7TjOS5I2iu07RhmmxmvrSmou6bUkS7/PyjM6xY7vDHNyTqyQMAB4ITZtA3Qzs9kKJcR/MrNeCkUh35A0jpC+sRMh58FGhPpl95QbtylwF9A3jrWBmX0fk5ovMLPrYr9HgKFm9rpC5d+xhJpjlwKvm9llcaPEiSlkX2mOFG/xdUIWNJP0Z+A84BxCeaBTzeyNWDliUZxjrJldES36KrtTnMLFFa2TS9aWNDW+/i8hC9f2wAQzSyQX3x3YMuF/JZTW2ZRQkXd4TAo+T9IrKcbfjpDHdjZAIkdCCnYFumh5abH1JK0b5zgg3vucpFQZzDKZoxXwmKTmhPSOiff2BvAvSQ8DT5nZXEkTgXsUaqE9Y2ZTU4znFDnuOnByScJH293M/mJmi2P7r0l9BPwlqV87MxsXr1W2u0YZ9IHwe//HpDlamtkvNTjHzYTS6FsAg4H6AGZ2FSGx+drAW5I6m9lrBAVfAjwo6egM5HeKDFe0TqExFjg5WnhI2kzSOsBrwGHRh9sc2CnFvf8DdpTULt6b+Fr/C7BuUr9xwGmJE0nd48vXgCNi2wBCLtxM50hmfYLiBBiUNE8HM3vfzK4GJgGdJW0CfGNmdxEs/K1TjOcUOa5onULj3wT/6xRJHwB3EFxcTxMqL7xPKIf+n/I3mtm3BJ/nU5LeBR6Ll0YB+ycWw4DTgZ5xsW0ay6Mf/g70lTSF4ML4fBXmSGYIMELSf4H5Se1nxgWvd4GFwPOEiIipkt4hJDq/sfJH5BQbnuvAcRwny7hF6ziOk2Vc0TqO42QZV7SO4zhZxhWt4zhOlnFF6ziOk2Vc0TqO42QZV7SO4zhZ5v8BPz7oEv23XrsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_confusion_matrix(cm, classes_map,\n",
    "                          normalize=False,\n",
    "                          title='Confusion Matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap = cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes_map))\n",
    "    inv_map = {v: k for k, v in classes_map.items()}\n",
    "    labels = inv_map.values()\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    if normalize:\n",
    "        cm = np.around(cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis], 2)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color = \"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "\n",
    "\n",
    "validation_classes = []\n",
    "validation_images = []\n",
    "\n",
    "for i in range( -(-validation_generator.samples // validation_generator.batch_size)):\n",
    "    batch = validation_generator.next()\n",
    "    expected = np.argmax(batch[1], axis = 1) \n",
    "    validation_classes.extend(expected)\n",
    "    validation_images.extend(batch[0])\n",
    "\n",
    "validation_classes = np.array(validation_classes)\n",
    "validation_images = np.array(validation_images)\n",
    "\n",
    "Y_pred = model.predict(validation_images)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "\n",
    "print(classification_report(validation_classes, y_pred,\n",
    "                            target_names = ['CHIN', 'MOUTH_CHIN', 'NOSE_MOUTH', 'CORRECT', 'MASKOFF']))\n",
    "\n",
    "cfs_mt = confusion_matrix(validation_classes, y_pred)\n",
    "classes = {'CHIN': 0, 'MOUTH_CHIN': 1, 'NOSE_MOUTH': 2, 'CORRECT': 3, 'MASKOFF': 4}\n",
    "plot_confusion_matrix(cfs_mt, classes_map = classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time Detection Using OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f8591fc18c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7f8591fc18c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rw/pxymp48w83d24hrnsbxpzdjw000gvn/T/ipykernel_8769/231555786.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = ConfigProto()\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "keras_model = tf.keras.models.load_model('/Users/nguyen_l5/Downloads/model_weights_changingDataGen')\n",
    "#keras_model = load_model('model.h5')\n",
    "classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')\n",
    "cam = cv2.VideoCapture(0) \n",
    "\n",
    "\n",
    "\n",
    "label = {\n",
    "    0: {\"name\": \"Mask is not covering nose and mouth\", \"color\": (51, 153, 255), \"id\": 0},\n",
    "    1: {\"name\": \"Mash is not covering nose\", \"color\": (255, 255, 0), \"id\": 1},\n",
    "    2: {\"name\": \"Mask is not covering chin\", \"color\": (255, 153, 255), \"id\": 2},\n",
    "    3: {\"name\": \"Mask is worn correctly\", \"color\": (0, 102, 51), \"id\": 3},\n",
    "    4: {\"name\": \"No mask\", \"color\": (0,0,255), \"id\": 4},\n",
    "}\n",
    "\n",
    "\n",
    "while True:\n",
    "    status, frame = cam.read()\n",
    "\n",
    "    if not status:\n",
    "        break\n",
    "\n",
    "    if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "        break\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = classifier.detectMultiScale(gray)\n",
    "\n",
    "    for x,y,w,h in faces:\n",
    "        color = (0,0,0)\n",
    "        gray_face = gray[y:y+h+50, x:x+w+50]\n",
    "\n",
    "        if gray_face.shape[0] >= 200 and gray_face.shape[1] >= 200:\n",
    "\n",
    "            gray_face = cv2.resize(gray_face, (300, 300))\n",
    "            gray_face = gray_face / 255\n",
    "            gray_face = np.expand_dims(gray_face, axis=0)\n",
    "            gray_face = gray_face.reshape((1, 300, 300, 1))\n",
    "            pred = np.argmax(keras_model.predict(gray_face))\n",
    "            classification = label[pred][\"name\"]\n",
    "            color = label[pred][\"color\"]\n",
    "\n",
    "            cv2.rectangle(frame, (x,y), (x+w, y+h), color, label[pred][\"id\"])\n",
    "\n",
    "            cv2.putText(frame, classification, (x, y + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n",
    "            cv2.putText(frame, f\"Number of detected faces: {len(faces)}\",(20,20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 2, cv2.LINE_AA)\n",
    "\n",
    "    \n",
    "    cv2.imshow(\"Cam\", frame)\n",
    "    \n",
    "    #Press Esc to stop the camera\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0Nck1YxQFw0p",
    "_2B_jAsHojLv",
    "MxfOi1h12uWp"
   ],
   "name": "correctly/incorrectly worn face mask detection (cleaned)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
